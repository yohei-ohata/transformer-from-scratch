{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71507c66",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf3f8e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fbee0",
   "metadata": {},
   "source": [
    "The Linear TransformationIn the context of modern neural networks, a linear layer (also known as a fully connected or dense layer) is typically represented as:\n",
    "\n",
    "$$y = xW^T + b$$\n",
    "\n",
    "**Component Breakdown**\n",
    "\n",
    "* $x$: The input tensor. In a Transformer, this is usually a vector of shape `[batch_size, sequence_length, input_features]`.\n",
    "* $W^T$: The transpose of the weight matrix.\n",
    "* $b$: The bias vector (optional), which is added to the result of the matrix multiplication.\n",
    "* $y$: The output tensor.\n",
    "<!-- $$\n",
    "\\displaystyle\n",
    "y = x W^T\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db989d5",
   "metadata": {},
   "source": [
    "### Implimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5dbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    __constants__ = [\"in_features\", \"out_features\"]\n",
    "    \n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,                   # input dimensions\n",
    "        out_features: int,                  # output dimensions\n",
    "        device: torch.device | None = None, # CPU or GPU\n",
    "        dtype: torch.dtype | None = None    # float32, 64, etc\n",
    "    ) -> None:\n",
    "        \n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(\n",
    "            torch.empty((out_features, in_features), **factory_kwargs)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.trunc_normal_(self.weight)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        return x @ self.weight.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6426eb",
   "metadata": {},
   "source": [
    "## Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d7b59",
   "metadata": {},
   "source": [
    "#### Definition `Linear` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# Inherit super class constructor `nn.Module`\n",
    "class Linear(nn.Module):\n",
    "\n",
    "    # these are fixed configuration values of the layer\n",
    "    __constants__ = [\"in_features\", \"out_features\"]\n",
    "    \n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    # weight is a PyTorch Tensor and stores the weight matrix\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,                   # input dimensions\n",
    "        out_features: int,                  # output dimensions\n",
    "        device: torch.device | None = None, # CPU or GPU\n",
    "        dtype: torch.dtype | None = None    # float32, 64, etc\n",
    "    ) -> None:\n",
    "        \n",
    "        # dictionary is later passed into torch.empty() **operator means: it unpacks the dictionary into keyword arguments\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        \n",
    "        # inherit parent class constructor\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Creating the Weight Matrix\n",
    "        # If you don't wrap it in Parameter, the optimizer won't update it. so it means trainable weight matrix\n",
    "        self.weight = Parameter(\n",
    "            torch.empty((out_features, in_features), **factory_kwargs)\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0b56e",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6adfcf",
   "metadata": {},
   "source": [
    "**`trunc_normal_`** initialization\n",
    "\n",
    "![Truncated Normal Distribution](/home/yohei.ohata.ee/Projects/stanford-cs336/assignment1-basics/cs336_basics/images/TruncatedNormal.png)\n",
    "\n",
    "Fill the input Tensor with values drawn from a truncated normal distribution.\n",
    "\n",
    "The values are effectively drawn from the normal distribution $ \\mathcal{N}(\\textrm{mean},\\textrm{std}^2)$ with values outside $[a,b]$ redrawn until they are within the bounds. \n",
    "\n",
    "The method used for generating the random values works best when $a≤\\textrm{mean}≤b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self) -> None:\n",
    "    nn.init.trunc_normal_(self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ff44e",
   "metadata": {},
   "source": [
    "### Matrix Multiplation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor) -> Tensor:\n",
    "    return x @ self.weight.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790a99a",
   "metadata": {},
   "source": [
    "#### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24aac4",
   "metadata": {},
   "source": [
    "**PyTorch Weight Storage & Matrix Multiplication**\n",
    "\n",
    "In PyTorch, linear layers typically store weights in the shape `(out_features, in_features)`. \n",
    "While it might seem counterintuitive compared to the standard mathematical notation $y = W x$, <br>\n",
    "there are significant hardware advantages to this approach.\n",
    "\n",
    "> **Note:** Even if you stored the weight as `(in_features, out_features)`, the operation `x @ self.weight` would technically work, but it wouldn't benefit from the same level of low-level optimization.\n",
    "\n",
    "\n",
    "**Memory Layout and GEMM**\n",
    "\n",
    "* **Storage Shape:** `(out_features, in_features)` row-major form\n",
    "* **Execution:** When you run `x @ weight.T` which is row-form major, PyTorch leverages highly optimized routines that maximize cache hits.\n",
    "\n",
    "---\n",
    "\n",
    "**Memory Layout: Row-Major vs. Column-Major**\n",
    "\n",
    "The way a language stores a 2D array in linear memory (RAM) dictates how fast certain operations will be.\n",
    "\n",
    "![Row-Major vs. Column-Major](/home/yohei.ohata.ee/Projects/stanford-cs336/assignment1-basics/cs336_basics/images/row-major-column-major.webp)\n",
    "\n",
    "\n",
    "**Comparison Table**\n",
    "\n",
    "| Feature | Row-Major Order | Column-Major Order |\n",
    "| --- | --- | --- |\n",
    "| **Storage Logic** | Elements are stored row-by-row. | Elements are stored column-by-column. |\n",
    "| **Adjacent Elements** |  and  are neighbors. |  and  are neighbors. |\n",
    "| **Performance** | Faster row-wise access. | Faster column-wise access. |\n",
    "| **Primary Use** | General purpose programming. | Scientific & Mathematical computing. |\n",
    "\n",
    "**Language Ecosystems**\n",
    "\n",
    "| Type | Key Languages |\n",
    "| --- | --- |\n",
    "| **Row-Major** (C-Style) | C, C++, Python (NumPy default), Pascal, SAS, HLSL |\n",
    "| **Column-Major** (Fortran-Style) | Fortran, MATLAB, R, Julia, Scilab, OpenGL (GLSL) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dba790",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0395cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([4, 8])\n",
      "Input Tensor:\n",
      "tensor([[-1.6999, -0.4738, -0.5925,  0.4202,  0.1482,  0.4658, -1.4073, -1.0404],\n",
      "        [ 0.6700,  0.9281, -1.8762,  0.3649,  0.5327, -1.8633,  0.0222,  0.6440],\n",
      "        [ 1.4492,  0.6457,  0.0057, -0.6342,  0.8637,  0.4045,  0.0803, -0.0030],\n",
      "        [ 2.4281,  0.1656,  0.7150, -1.2204, -2.6843,  0.8813, -1.1786,  0.4195]])\n",
      "\n",
      "Weight Shape: torch.Size([4, 8])\n",
      "Weight Tensor:\n",
      "Parameter containing:\n",
      "tensor([[ 1.7370,  0.4927, -0.4564, -0.9130,  0.8827,  0.0034,  0.6451,  0.0218],\n",
      "        [ 1.3956, -0.2552,  0.8936, -0.4262, -0.3371, -0.3544,  0.1371,  0.9575],\n",
      "        [ 0.7492,  0.0960, -1.3545, -0.7660,  0.8891, -1.1542,  0.8908,  0.2677],\n",
      "        [ 0.1911,  0.5428, -0.2852,  0.9995, -0.6744,  0.6710,  0.3653, -0.5579]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Output Shape: torch.Size([4, 4])\n",
      "Output Tensor:\n",
      "tensor([[-4.0976, -4.3640, -2.7765,  0.2859],\n",
      "        [ 2.6365, -0.0334,  5.6693, -0.4291],\n",
      "        [ 4.2273,  1.7066,  1.9976, -0.2881],\n",
      "        [ 1.9696,  5.3380, -2.5398,  0.8671]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup dimensions\n",
    "batch_size = 4\n",
    "in_features = 8\n",
    "out_features = 4\n",
    "\n",
    "# 2. Instantiate your custom layer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Linear(in_features, out_features, device=device)\n",
    "\n",
    "# 3. Create a dummy input tensor\n",
    "# Shape: (Batch Size, In Features)\n",
    "input_tensor: Float[Tensor, \"batch in_features\"] = torch.randn(batch_size, in_features).to(device)\n",
    "\n",
    "# 4. Run the forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# 5. Check the results\n",
    "print(f\"Input Shape:  {input_tensor.shape}\")\n",
    "print(\"Input Tensor:\")\n",
    "print(input_tensor)\n",
    "print(f\"\\nWeight Shape: {model.weight.shape}\")\n",
    "print(\"Weight Tensor:\")\n",
    "print(model.weight)\n",
    "print(f\"\\nOutput Shape: {output.shape}\")\n",
    "print(\"Output Tensor:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
