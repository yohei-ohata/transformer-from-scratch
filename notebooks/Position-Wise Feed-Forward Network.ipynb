{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6fed6a",
   "metadata": {},
   "source": [
    "# Position-Wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4e253",
   "metadata": {},
   "source": [
    "This section covers the **Position-Wise Feed-Forward Network**. \n",
    "Modern language models tend to two main changes:\n",
    "\n",
    "* they use another activation function and\n",
    "* employ a gating mechanism\n",
    "\n",
    "We should understand three key areas: \n",
    "* **standard feed-forward structures (MLP/FFN)**, \n",
    "* **activation functions(ReLU/SiLU)**, and, \n",
    "* **Gated Linear Units (GLU)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f71920",
   "metadata": {},
   "source": [
    "## Feed-Forward Network(FFN) & Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714865ed",
   "metadata": {},
   "source": [
    "In the world of machine learning, **Feedforward Neural Networks (FFN)** and **Multilayer Perceptrons (MLP)** are often used interchangeably. However, while they are closely related, they represent different levels of abstraction.\n",
    "\n",
    "The simplest way to distinguish them: **All MLPs are FFNs, but not all FFNs are MLPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a631a0",
   "metadata": {},
   "source": [
    "### 1. Feedforward Neural Network (FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15777450",
   "metadata": {},
   "source": [
    "An FFN is a broad category of neural networks where information moves in only one direction: forward. \n",
    "\n",
    "There are **no cycles or loops** (FFN <-> RNN).\n",
    "\n",
    "```\n",
    "data -> input nodes -> hidden layer -> ... -> hidden layer -> output\n",
    "```\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "A feedforward network can be viewed as a composition of functions. For a network with  layers, the output  is calculated as:\n",
    "\n",
    "$$\n",
    "y = f^{(L)}(f^{(L-1)}(...f^{(1)}(x)...))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the input vector.\n",
    "* $f^{(i)}$ represents the transformation at layer $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21156d7e",
   "metadata": {},
   "source": [
    "### 2. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fca83",
   "metadata": {},
   "source": [
    "\n",
    "An MLP is a kind of modern of FFN. To be classified as an MLP, a network must meet three criteria:\n",
    "\n",
    "1. **Multiple Layers:** It must have at least one hidden layer (3 layers total: input, hidden, and output).\n",
    "2. **Fully Connected (Dense):** Every neuron in layer  must connect to every neuron in layer $i+1$.\n",
    "3. **Non-linear Activations:** It must use non-linear activation functions (like ReLU or Sigmoid) to avoid collapsing into a simple linear model.\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "For a single layer in an MLP, the output vector  is calculated as:\n",
    "\n",
    "$$\n",
    "h = \\sigma(W x + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $x \\in \\mathbb{R}^n$: Input vector.\n",
    "* $W \\in \\mathbb{R}^{m \\times n}$: Weight matrix.\n",
    "* $b \\in \\mathbb{R}^m$: Bias vector.\n",
    "* $\\sigma$: Non-linear activation function (e.g., $ReLU(z) = \\max(0, z)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05888a14",
   "metadata": {},
   "source": [
    "### 3. Direct Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833bc61",
   "metadata": {},
   "source": [
    "\n",
    "| Feature | Feedforward Neural Network (FFN) | Multilayer Perceptron (MLP) |\n",
    "| --- | --- | --- |\n",
    "| **Scope** | A broad class of architectures. | A specific subset of FFNs. |\n",
    "| **Connectivity** | Can be sparse or locally connected (e.g., CNNs). | **Must** be fully connected (Dense). |\n",
    "| **Structure** | Unidirectional flow (no loops). | Unidirectional flow with  hidden layer. |\n",
    "| **Complexity** | Varies from a single layer to billions. | Requires a hidden layer to solve non-linear problems (XOR). |\n",
    "\n",
    "**Key Distinction: The \"XOR\" Problem**\n",
    "\n",
    "Historically, a \"Single-Layer Perceptron\" (an FFN with no hidden layer) could only solve linearly separable problems. It could not solve the **XOR** logic gate because it couldn't draw a non-linear boundary.\n",
    "\n",
    "By adding a hidden layer and non-linear activations, it becomes an **MLP**, which gains the power of the **Universal Approximation Theorem**: the ability to approximate any continuous function given enough neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8aae2e",
   "metadata": {},
   "source": [
    "## Activation Functions (ReLU, SiLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095db4d6",
   "metadata": {},
   "source": [
    "<img src=\"../images/SiLU_ReLU.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190919e4",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00b869",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Characteristics**\n",
    "\n",
    "* It turns off neurons that have negative values (sets them to 0). \n",
    "* ‚úÖ pros\n",
    "    * The network becomes lighter and more efficient.\n",
    "* ‚ùå„ÄÄcons\n",
    "    * **The \"Dying ReLU\" Problem:** because of the ReLU turning negative value into zero, the gradient becomes 0, it will stay at 0 forever and that neuron \"dies.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ac7b0",
   "metadata": {},
   "source": [
    "### 2. SiLU (Sigmoid Linear Unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ae094",
   "metadata": {},
   "source": [
    "Also known as **Swish**, SiLU is a more modern, \"smooth\" version of ReLU. \n",
    "\n",
    "$$\n",
    "SiLU(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "* Characteristics\n",
    "* Unlike ReLU, which has a sharp \"elbow\" at zero, SiLU is smooth everywhere. \n",
    "\n",
    "* ‚úÖPros\n",
    "    * This helps the optimization process (Gradient Descent) find better minima.\n",
    "    * Interestingly, for small negative values, SiLU actually dips below zero before returning to zero. This allows some negative information to flow through, which often leads to better accuracy than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bf91f",
   "metadata": {},
   "source": [
    "### 3. Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce77be",
   "metadata": {},
   "source": [
    "\n",
    "| Feature | ReLU | SiLU (Swish) |\n",
    "| --- | --- | --- |\n",
    "| **Formula** | $\\max(0, x)$ | $x \\cdot \\text{sigmoid}(x)$ |\n",
    "| **Differentiable** | Not at $x=0$ | Yes, everywhere |\n",
    "| **Computation** | Extremely fast (simple comparison) | Moderate (requires exponential) |\n",
    "| **Output Range** | $[0, \\infty)$ | $[\\approx -0.28, \\infty)$ |\n",
    "| **Best For** | General MLPs, CNNs | Deep Transformers, YOLO, LLMs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a59732",
   "metadata": {},
   "source": [
    "## Gated Linear Units (GLUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c8d2a",
   "metadata": {},
   "source": [
    "The **Gated Linear Unit (GLU)** is a sophisticated architectural component that moves away from simple \"all-or-nothing\" activations (like ReLU) toward a **gating mechanism**.\n",
    "\n",
    "The original definition by Dauphin et al. is:\n",
    "\n",
    "$$\n",
    "\\text{GLU}(x, W_1, W_2) = \\sigma({W_1}x) \\odot ({W_2}x)\n",
    "$$\n",
    "\n",
    "To visualize what's happening, let's break it into two parallel paths:\n",
    "\n",
    "1. **The Gate $\\sigma({W_1}x)$:** This path applies a sigmoid function, squashing the linear transformation into a range of $[0,1]$. It acts as a learned \"filter.\"\n",
    "2. **The Content $({W_2}x)$:** This is a standard linear transformation of the input. It carries the actual \"data\" or features.\n",
    "3. **The Element-wise Product ($\\odot$):** The gate vector multiplies the content vector. If the gate value is $1.0$, the content passes through perfectly; if it's $0.0$, the content is blocked.\n",
    "\n",
    "---\n",
    "\n",
    "**üîçWhy use GLUs?**\n",
    "\n",
    "* **Vanishing Gradient Relief:** In a standard network, gradients must pass through non-linearities (like Tanh) at every layer, which can shrink the signal. In a GLU, if the gate is \"open\" (near 1), the gradient flows through the  path linearly, preserving its strength.\n",
    "* **Dynamic Selection:** Unlike ReLU, a GLU can choose to block or pass *any* feature based on the context of the input.\n",
    "* **Reduced Training Bias:** Because they have a linear path, they are easier to train in very deep stacks compared to pure Sigmoid or Tanh networks.\n",
    "\n",
    "---\n",
    "\n",
    "**The \"SwiGLU\" Evolution**\n",
    "\n",
    "Researchers found that replacing the **Sigmoid** with a **SiLU** (Swish) activation works significantly better.However, we offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.\n",
    "\n",
    "The **SwiGLU** variant is defined as:\n",
    "\n",
    "$$\\text{SwiGLU}(x, {W_1}, {W_2}) = \\text{SiLU}({W_1}x) \\otimes ({W_2}x)$$\n",
    "\n",
    "In this version, the \"gate\" isn't just a 0-to-1 filter; it‚Äôs a smooth, non-monotonic function that allows the network to learn much more complex representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d54ed",
   "metadata": {},
   "source": [
    "## Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
