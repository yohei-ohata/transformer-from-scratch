{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8feaf7",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention (SPDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5451f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddba99",
   "metadata": {},
   "source": [
    "The **Scaled Dot-Product Attention** mechanism can be conceptualized as an associative retrieval system where the **Query** represents a specific semantic \"question\" directed toward **Keys**, which serve as indexing metadata for the underlying **Values**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f4375",
   "metadata": {},
   "source": [
    "## Step-by-Step Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a7455",
   "metadata": {},
   "source": [
    "### Attention mechanism as Library Information Retrieval Metaphor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365e758",
   "metadata": {},
   "source": [
    "Let us define our variables within the library context:\n",
    "\n",
    "<img src = \"../images/SelfAttention.png\">\n",
    "\n",
    "* $Q$ **(Query)**: Your search intent (e.g., \"How does backpropagation work?\").\n",
    "* $K$ **(Key)**: The spine labels or metadata of all books on the shelf (e.g., \"Calculus,\" \"Optimization,\" \"Neural Networks\").\n",
    "* $V$ **(Value)**: The actual instructional content inside those books.\n",
    "\n",
    "**The Three-Stage Retrieval Process**\n",
    "\n",
    "1. Similarity Computation: <br>\n",
    "    You compare your question ($Q$) against every label on the bookshelf ($K$) by `MatMul` operation. This is a \"matching\" phase. If your question is about ML, the dot product will yield a high score for the \"Neural Networks\" book.\n",
    "\n",
    "2. Scaling: <br>\n",
    "    When representations have very high dimensionality, the raw matching score between a query and a key (paticular book) could become so large that it overwhelms all other candidates. To prevent this, Scaling serves as a stabilizing adjstment.\n",
    "\n",
    "3. Normalization and Aggregation: <br>\n",
    "    In this process, transformation, produces what are commonly referred as **attention weights**, which behave like probabilities. For example, if on source receives a weight of $0.7$ and another $0.2$, the system does not exclusively select the highest-scoring source. Instead, it intergrates information proportionally. In effect the final representation becomes a weighted synthesis: approximately $70%$ of the informational contribution derives from the primary source, while $20%$ is drawn from a secondary but still relevant source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370fb92",
   "metadata": {},
   "source": [
    "### Mathematical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d9f2e",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times d_{model}}$ represent your input sequence, where $n$ is the sequence length and $d_{model}$ is the embedding dimension. To generate the Query ($Q$), Key ($K$), and Value ($V$) matrices, we perform three independent linear transformations:\n",
    "\n",
    "$$\n",
    "Q = X W_q, \\quad K = X W_k, \\quad V = X W_v\n",
    "$$\n",
    "\n",
    "Where the weight matrices are defined as:\n",
    "\n",
    "* $W_q, W_k \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "* $W_v \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
    "\n",
    "**Random Initialization**\n",
    "\n",
    "for initialization, we use **Variance Scaling Initialization**, such as **Xavier (Glorot) Initialization**:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{d_{in} + d_{out}}\\right)\n",
    "$$\n",
    "\n",
    "This ensures the variance of the activations remains constant across layers, preventing the signal from vanishing or exploding as it traverses the network.\n",
    "\n",
    "---\n",
    "\n",
    "**Positional Information Injection**\n",
    "\n",
    "We then apply the position-dependent rotation:\n",
    "\n",
    "* **Rotated Query**: $Q' = \\{ R_i q^{(i)} \\}_{i=1}^n$\n",
    "* **Rotated Key**: $K' = \\{ R_j k^{(j)} \\}_{j=1}^n$\n",
    "\n",
    "---\n",
    "\n",
    "**The Attention Operation**\n",
    "\n",
    "The attention output is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q', K', V) = \\text{softmax}\\left(\\frac{Q' (K')^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**The Three Stages of Computation**\n",
    "\n",
    "1. **Similarity Computation (Rotary Dot Product)**: <br>\n",
    "\n",
    "    The product $Q' (K')^\\top$ calculates the similarity between the rotated query at position $i$ and the rotated key at position $j$. Because $R$ is an orthogonal transformation, the score $\\langle R_i q^{(i)}, R_j k^{(j)} \\rangle$ is equivalent to a rotation of the relative distance $(j-i)$, making the attention position-sensitive.\n",
    "\n",
    "2. **Scaling**:<br>\n",
    "\n",
    "    The scores are divided by $\\sqrt{d_k}$. This normalization is critical; without it, the dot products of high-dimensional vectors (especially after rotation) could reach extreme magnitudes, pushing the softmax into regions with **vanishing gradients**.\n",
    "\n",
    "3. **Normalization and Aggregation**:<br>\n",
    "\n",
    "    The row-wise softmax converts these relative-distance-aware scores into a probabilistic distribution. Finally, we multiply by $V$ (which is not rotated, as value vectors represent content rather than position) to aggregate information.\n",
    "\n",
    "---\n",
    "**Causal Masking**\n",
    "\n",
    "Since the dot-product ($QK^\\top$) employes `MatMul` operations, which enables it to look at all word tokens within the sequences, we introduce **Causal Masking** that ensures them forbidden from glacncing at any token to the right of their current hand position.\n",
    "\n",
    "We introduce a mask matrix $M$ into the similarity scores before the softmax operation.\n",
    "\n",
    "The Masked Formula represents :\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top + M}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where $M$ is a matrix of the same shape as the attention scores ($n \\times n$). \n",
    "\n",
    "For a causal (auto-regressive) model, the elements of $M$ are defined as:\n",
    "\n",
    "$$\n",
    "M_{ij} = \\begin{cases} 0 & \\text{if } i \\geq j \\\\ -\\infty & \\text{if } i < j \\end{cases}\n",
    "$$\n",
    "\n",
    "When we add $-\\infty$ to the scores where $i < j$ (the \"future\" tokens), and then apply the softmax function:\n",
    "\n",
    "$$\n",
    "e^{-\\infty} = 0\n",
    "$$\n",
    "\n",
    "The attention weights for all future tokens become exactly zero. Consequently, the Query at position $i$ can only \"see\" and aggregate information from Keys at positions $0$ to $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0bc25",
   "metadata": {},
   "source": [
    "### Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d4cb0",
   "metadata": {},
   "source": [
    "#### Step0. Preparation\n",
    "\n",
    "* String Sequence: `\"the cat ate the rat\"`\n",
    "* Token Sequence: `[9, 0, 2, 7, 0, 7, 3, 9, 0, 6, 7]`\n",
    "* Scenario: Let's observe how the Query for the token `c` (Position 2, ID `2`) attends to the Key for the token `r` (Position 9, ID `6`).\n",
    "\n",
    "#### Step1. Projecting BPE Sequences\n",
    "\n",
    "**The Starting Point ($X$)**\n",
    "\n",
    "The ID `2` is converted into a vector $x^{(2)} \\in \\mathbb{R}^{d_{model}}$. Letâ€™s assume $d_{model} = 512$.\n",
    "\n",
    "$$\n",
    "x^{(2)} = [0.15, -0.02, \\dots, 0.88]\n",
    "$$\n",
    "\n",
    "**The Projection ($W_q, W_k, W_v$)**\n",
    "\n",
    "1. **Generate Query**: $q^{(2)} = x^{(2)} \\cdot W_q$. \n",
    "\n",
    "    This vector represents what the `c` token is **looking for** in other tokens.\n",
    "\n",
    "2. **Generate Key**: $k^{(2)} = x^{(2)} \\cdot W_k$. \n",
    "\n",
    "    This represents what **information** the `c` token offers to others.\n",
    "\n",
    "3. **Generate Value**: $v^{(2)} = x^{(2)} \\cdot W_v$. \n",
    "\n",
    "    This contains the **actual content** that will be passed to the next layer if this token is selected.\n",
    "\n",
    "#### Step 2: Similarity (The \"Search\")\n",
    "\n",
    "The model computes the dot product between the rotated Query vector of `c` ($q'^{(2)}$) and the rotated Key vector of `r` ($k'^{(9)}$).\n",
    "\n",
    "* **With RoPE**: \n",
    "\n",
    "    Because $q^{(2)}$ was rotated by $\\theta_2$ and $k^{(9)}$ was rotated by $\\theta_9$, the dot product inherently \"measures\" the gap of 7 positions. If the model has learned that a `c` and an `r` separated by 7 tokens are syntactically linked (e.g., in the words `cat` and `rat`), this score will be high.\n",
    "\n",
    "#### Step 3: Scaling (The \"Stability\")\n",
    "\n",
    "Suppose the dot product result is $80.0$. If $d_k = 64$, we divide by $\\sqrt{64} = 8$.\n",
    "\n",
    "$$\n",
    "\\text{Score} = 80.0 / 8 = 10.0\n",
    "$$\n",
    "\n",
    "This prevents the softmax from \"peaking\" too early, ensuring the model can still consider other tokens in the sequence (like the `a` or `t`).\n",
    "\n",
    "#### Step 4: Aggregation (The \"Context\")\n",
    "\n",
    "The softmax assigns a high weight (e.g., $0.85$) to the `r` at position `9`. The final representation for `c` will now contain a heavy \"dosage\" of the Value vector for `r`.\n",
    "\n",
    "**Result**: \n",
    "\n",
    "The representation of `cat` is now \"aware\" of the upcoming `rat` because of their relative spatial configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc54e85",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    " \n",
    "        self.softmax = F.softmax(dim=-1)\n",
    " \n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            Q : torch.Tensor,\n",
    "            K : torch.Tensor,\n",
    "            V : torch.Tensor,\n",
    "            mask : torch.Tensor\n",
    "        ) -> Tensor:\n",
    " \n",
    "        *batch_size, sew_len, d_k = Q.shape\n",
    "        # or d_k = Q.size(-1)\n",
    " \n",
    "        atten_score = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    " \n",
    "        # mask is given hiper parameter two purposes, mathematically, limit x -> 0\n",
    "        if mask is not None:\n",
    "            atten_score = atten_score.masked_fill(mask == 0, float(\"-inf\"))\n",
    " \n",
    "        atten_score = self.softmax(atten_score)\n",
    " \n",
    "        Output = atten_score @ V\n",
    " \n",
    "        return Output    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
