{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f7b5ae",
   "metadata": {},
   "source": [
    "# Relative Positionla Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de2a75",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06210902",
   "metadata": {},
   "source": [
    "<img src = \"../images/RoPE.png\" width = \"100%\">\n",
    "\n",
    "**Rotary Posisional Embeddings or RoPE** represent a paradigm shift in sequence modeling by unifying absolute and relative positional information through geometric transformations.\n",
    "\n",
    "The mathematical brilliance of RoPE lies in the **Dot Product Linearity**. When the self-attention mechanism calculates the score between a Query ($q$) at position $i$ and a Key ($k$) at position $j$, the result depends only on the relative angle between them: $\\theta_{i} - \\theta_{j}$.\n",
    "\n",
    "Because the dot product of two rotated vectors is invariant to their absolute rotation and only sensitive to their relative displacement, the model \"senses\" how far apart two tokens are by the degree of rotation needed to align them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43907b7a",
   "metadata": {},
   "source": [
    "## Step by Step Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21215c85",
   "metadata": {},
   "source": [
    "### Mathematical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a230bd",
   "metadata": {},
   "source": [
    "The fundamental objective of RoPE is to encode position $i$ by rotating the Query ($q$) and Key ($k$) vectors in a manner that preserves their relative distance.\n",
    "\n",
    "**The Rotation Mechanism**\n",
    "\n",
    "For a hidden dimension $d$, we treat the vector as $d/2$ pairs of coordinates. \n",
    "Assume for simplicity that our model dimension $d=4$.\n",
    "\n",
    "$$\n",
    "q = [q_1, q_2, q_3, q_4]\n",
    "$$\n",
    "\n",
    "We treat the vector as **two independent 2D planes**\n",
    "\n",
    "* Plane 1: $(q_1, q_2)$\n",
    "* Plane 2: $(q_3, q_4)$\n",
    "\n",
    "For each pair $k \\in \\{1, \\dots, d/2\\}$, we define a position-dependent angle:\n",
    "\n",
    "$$\n",
    "\\theta_{i,k} = i \\cdot \\Theta^{-2(k-1)/d}\n",
    "$$\n",
    "\n",
    "As shown in the example above, we calculate two unique angles ($\\theta_{i,1}$ and $\\theta_{i,2}$).\n",
    "\n",
    "$$\n",
    "\\theta_{i,1} = i \\cdot \\Theta^{-2(1-1)/d} = i \\\\\n",
    "\\theta_{i,2} = i \\cdot \\Theta^{-2(2-1)/d} = i \\cdot \\Theta^{-2/d}\n",
    "$$\n",
    "\n",
    "\n",
    "The rotation for each pair is governed by the $2 \\times 2$ matrix $R_{i,k}$:\n",
    "\n",
    "$$\n",
    "R_{i,k} = \\begin{bmatrix} \\cos(\\theta_{i,k}) & -\\sin(\\theta_{i,k}) \\\\ \\sin(\\theta_{i,k}) & \\cos(\\theta_{i,k}) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**The Full Transformation**\n",
    "\n",
    "These blocks are assembled into a block-diagonal matrix $R_i$, which acts on the entire embedding vector:\n",
    "\n",
    "$$\n",
    "R_i = \\begin{bmatrix} \n",
    "R_{i,1} & 0 & \\dots & 0 \\\\ 0 & R_{i,2} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & R_{i,d/2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The rotated vector is computed as $q'^{(i)} = R_i q^{(i)}$. Crucially, when calculating attention between positions $i$ and $j$, the dot product satisfies:\n",
    "\n",
    "$$\n",
    "\\langle R_i q^{(i)}, R_j k^{(j)} \\rangle = \\langle q^{(i)}, R_{j-i} k^{(j)} \\rangle\n",
    "$$\n",
    "\n",
    "This demonstrates that the attention score depends solely on the relative displacement $j-i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a53cf9",
   "metadata": {},
   "source": [
    "### Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856783b7",
   "metadata": {},
   "source": [
    "Let us apply this to your sequence. \n",
    "\n",
    "* String Sequence: `\"the cat ate the rat\"`\n",
    "* Token Sequence: `[9, 0, 2, 7, 0, 7, 3, 9, 0, 6, 7]`\n",
    "* Focus: \n",
    "    * Token 2 (the 'c' in 'cat') at Position $i=2$ vs. \n",
    "    * Token 6 (the 'r' in 'rat') at Position $i=9$.\n",
    "\n",
    "For clarity, we will assume a small embedding dimension of $d=2$ (one rotation plane) and a base constant $\\Theta = 10,000$.\n",
    "\n",
    "#### Step 1: Calculating the Angles ($\\theta$)\n",
    "\n",
    "For $i=2$ (Token 'c'):\n",
    "\n",
    "$$\n",
    "\\theta_{2} = 2 \\cdot 10000^{0} = 2 \\text{radians}\n",
    "$$\n",
    "\n",
    "For $i=9$ (Token 'r'):\n",
    "\n",
    "$$\n",
    "\\theta_{9} = 9 \\cdot 10000^{0} = 9 \\text{radians}\n",
    "$$\n",
    "\n",
    "#### Step 2: Constructing the Rotation Matrices\n",
    "\n",
    "For the 'c' token ($i=2$):\n",
    "\n",
    "$$\n",
    "R_2 = \\begin{bmatrix} \\cos(2) & -\\sin(2) \\\\ \\sin(2) & \\cos(2) \\end{bmatrix} \\approx \\begin{bmatrix} -0.416 & -0.909 \\\\ 0.909 & -0.416 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 3: Resulting Interaction\n",
    "\n",
    "When the model performs self-attention between the Query of 'c' ($q^{(2)}$) and the Key of 'r' ($k^{(9)}$), the resulting score is influenced by the angular difference:\n",
    "\n",
    "$$\n",
    "\\Delta\\theta = \\theta_9 - \\theta_2 = 7 \\text{ radians}\n",
    "$$\n",
    "\n",
    "The model \"perceives\" that the `'r'` in `'rat'` is exactly 7 positions ahead of the `'c'` in `'cat'`, allowing it to maintain the syntactic relationship between these subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0831c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        theta : float,\n",
    "        d_k : int,\n",
    "        max_seq_len : int,\n",
    "        device: torch.device | None = None,\n",
    "    )-> None:\n",
    "        \n",
    "        factory_kwargs = {\"device\": device}\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.theta = theta\n",
    "        self.d_k = d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self._build_cache(**factory_kwargs)\n",
    "\n",
    "    def _build_cache(self, device=None):\n",
    "        position = torch.arange(\n",
    "            self.max_seq_len,\n",
    "            device=device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        dim = torch.arange(\n",
    "            0,\n",
    "            self.d_k,\n",
    "            2,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        inv_freq = 1.0 / (self.theta ** (dim / self.d_k))\n",
    "        sinusoid_inp = position * inv_freq\n",
    "\n",
    "        sin = torch.sin(sinusoid_inp)\n",
    "        cos = torch.cos(sinusoid_inp)\n",
    "\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        token_positions: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "\n",
    "        torch._check(\n",
    "            dim % 2 == 0,\n",
    "            lambda: \"Embedding dimension must be even for RoPE\",\n",
    "        )\n",
    "\n",
    "        if token_positions.dim() == 1:\n",
    "            token_positions = token_positions.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        cos = self.cos[token_positions]\n",
    "        sin = self.sin[token_positions]\n",
    "\n",
    "        x1 = x[..., 0::2]\n",
    "        x2 = x[..., 1::2]\n",
    "\n",
    "        real = cos * x1 - sin * x2\n",
    "        imag = sin * x1 + cos * x2\n",
    "\n",
    "        # Re-interleave\n",
    "        x_out = torch.stack((real, imag), dim=-1)\n",
    "        x_out = x_out.flatten(-2)\n",
    "\n",
    "        return x_out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
