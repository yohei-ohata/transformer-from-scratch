{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6fed6a",
   "metadata": {},
   "source": [
    "# Position-Wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4e253",
   "metadata": {},
   "source": [
    "This section covers the **Position-Wise Feed-Forward Network**. \n",
    "Modern language models tend to two main changes:\n",
    "\n",
    "* they use another activation function and\n",
    "* employ a gating mechanism\n",
    "\n",
    "We should understand three key areas: \n",
    "* **standard feed-forward structures (MLP/FFN)**, \n",
    "* **activation functions(ReLU/SiLU)**, and, \n",
    "* **Gated Linear Units (GLU)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f71920",
   "metadata": {},
   "source": [
    "## Feed-Forward Network(FFN) & Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714865ed",
   "metadata": {},
   "source": [
    "In the world of machine learning, **Feedforward Neural Networks (FFN)** and **Multilayer Perceptrons (MLP)** are often used interchangeably. However, while they are closely related, they represent different levels of abstraction.\n",
    "\n",
    "The simplest way to distinguish them: **All MLPs are FFNs, but not all FFNs are MLPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a631a0",
   "metadata": {},
   "source": [
    "### 1. Feedforward Neural Network (FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15777450",
   "metadata": {},
   "source": [
    "An FFN is a broad category of neural networks where information moves in only one direction: forward. \n",
    "\n",
    "There are **no cycles or loops** (FFN <-> RNN).\n",
    "\n",
    "```\n",
    "data -> input nodes -> hidden layer -> ... -> hidden layer -> output\n",
    "```\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "A feedforward network can be viewed as a composition of functions. For a network with  layers, the output  is calculated as:\n",
    "\n",
    "$$\n",
    "y = f^{(L)}(f^{(L-1)}(...f^{(1)}(x)...))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the input vector.\n",
    "* $f^{(i)}$ represents the transformation at layer $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21156d7e",
   "metadata": {},
   "source": [
    "### 2. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fca83",
   "metadata": {},
   "source": [
    "\n",
    "An MLP is a kind of modern of FFN. To be classified as an MLP, a network must meet three criteria:\n",
    "\n",
    "1. **Multiple Layers:** It must have at least one hidden layer (3 layers total: input, hidden, and output).\n",
    "2. **Fully Connected (Dense):** Every neuron in layer  must connect to every neuron in layer $i+1$.\n",
    "3. **Non-linear Activations:** It must use non-linear activation functions (like ReLU or Sigmoid) to avoid collapsing into a simple linear model.\n",
    "\n",
    "**Mathematical Representation**\n",
    "\n",
    "For a single layer in an MLP, the output vector  is calculated as:\n",
    "\n",
    "$$\n",
    "h = \\sigma(W x + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $x \\in \\mathbb{R}^n$: Input vector.\n",
    "* $W \\in \\mathbb{R}^{m \\times n}$: Weight matrix.\n",
    "* $b \\in \\mathbb{R}^m$: Bias vector.\n",
    "* $\\sigma$: Non-linear activation function (e.g., $ReLU(z) = \\max(0, z)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05888a14",
   "metadata": {},
   "source": [
    "### 3. Direct Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833bc61",
   "metadata": {},
   "source": [
    "\n",
    "| Feature | Feedforward Neural Network (FFN) | Multilayer Perceptron (MLP) |\n",
    "| --- | --- | --- |\n",
    "| **Scope** | A broad class of architectures. | A specific subset of FFNs. |\n",
    "| **Connectivity** | Can be sparse or locally connected (e.g., CNNs). | **Must** be fully connected (Dense). |\n",
    "| **Structure** | Unidirectional flow (no loops). | Unidirectional flow with  hidden layer. |\n",
    "| **Complexity** | Varies from a single layer to billions. | Requires a hidden layer to solve non-linear problems (XOR). |\n",
    "\n",
    "**Key Distinction: The \"XOR\" Problem**\n",
    "\n",
    "Historically, a \"Single-Layer Perceptron\" (an FFN with no hidden layer) could only solve linearly separable problems. It could not solve the **XOR** logic gate because it couldn't draw a non-linear boundary.\n",
    "\n",
    "By adding a hidden layer and non-linear activations, it becomes an **MLP**, which gains the power of the **Universal Approximation Theorem**: the ability to approximate any continuous function given enough neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8aae2e",
   "metadata": {},
   "source": [
    "## Activation Functions (ReLU, SiLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095db4d6",
   "metadata": {},
   "source": [
    "<img src=\"../images/SiLU_ReLU.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190919e4",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00b869",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Characteristics**\n",
    "\n",
    "* It turns off neurons that have negative values (sets them to 0). \n",
    "* âœ… pros\n",
    "    * The network becomes lighter and more efficient.\n",
    "* âŒã€€cons\n",
    "    * **The \"Dying ReLU\" Problem:** because of the ReLU turning negative value into zero, the gradient becomes 0, it will stay at 0 forever and that neuron \"dies.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ac7b0",
   "metadata": {},
   "source": [
    "### 2. SiLU (Sigmoid Linear Unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ae094",
   "metadata": {},
   "source": [
    "Also known as **Swish**, SiLU is a more modern, \"smooth\" version of ReLU. \n",
    "\n",
    "$$\n",
    "SiLU(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "* Characteristics\n",
    "* Unlike ReLU, which has a sharp \"elbow\" at zero, SiLU is smooth everywhere. \n",
    "\n",
    "* âœ…Pros\n",
    "    * This helps the optimization process (Gradient Descent) find better minima.\n",
    "    * Interestingly, for small negative values, SiLU actually dips below zero before returning to zero. This allows some negative information to flow through, which often leads to better accuracy than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bf91f",
   "metadata": {},
   "source": [
    "### 3. Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce77be",
   "metadata": {},
   "source": [
    "\n",
    "| Feature | ReLU | SiLU (Swish) |\n",
    "| --- | --- | --- |\n",
    "| **Formula** | $\\max(0, x)$ | $x \\cdot \\text{sigmoid}(x)$ |\n",
    "| **Differentiable** | Not at $x=0$ | Yes, everywhere |\n",
    "| **Computation** | Extremely fast (simple comparison) | Moderate (requires exponential) |\n",
    "| **Output Range** | $[0, \\infty)$ | $[\\approx -0.28, \\infty)$ |\n",
    "| **Best For** | General MLPs, CNNs | Deep Transformers, YOLO, LLMs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a59732",
   "metadata": {},
   "source": [
    "## Gated Linear Units (GLUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c8d2a",
   "metadata": {},
   "source": [
    "The **Gated Linear Unit (GLU)** is a sophisticated architectural component that moves away from simple \"all-or-nothing\" activations (like ReLU) toward a **gating mechanism**.\n",
    "\n",
    "The original definition by Dauphin et al. is:\n",
    "\n",
    "$$\n",
    "\\text{GLU}(x, W_1, W_2) = \\sigma({W_1}x) \\odot ({W_2}x)\n",
    "$$\n",
    "\n",
    "To visualize what's happening, let's break it into two parallel paths:\n",
    "\n",
    "1. **The Gate $\\sigma({W_1}x)$:** This path applies a sigmoid function, squashing the linear transformation into a range of $[0,1]$. It acts as a learned \"filter.\"\n",
    "2. **The Content $({W_2}x)$:** This is a standard linear transformation of the input. It carries the actual \"data\" or features.\n",
    "3. **The Element-wise Product ($\\odot$):** The gate vector multiplies the content vector. If the gate value is $1.0$, the content passes through perfectly; if it's $0.0$, the content is blocked.\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”Why use GLUs?**\n",
    "\n",
    "* **Vanishing Gradient Relief:** In a standard network, gradients must pass through non-linearities (like Tanh) at every layer, which can shrink the signal. In a GLU, if the gate is \"open\" (near 1), the gradient flows through the  path linearly, preserving its strength.\n",
    "* **Dynamic Selection:** Unlike ReLU, a GLU can choose to block or pass *any* feature based on the context of the input.\n",
    "* **Reduced Training Bias:** Because they have a linear path, they are easier to train in very deep stacks compared to pure Sigmoid or Tanh networks.\n",
    "\n",
    "---\n",
    "\n",
    "**The \"SwiGLU\" Evolution**\n",
    "\n",
    "Researchers found that replacing the **Sigmoid** with a **SiLU** (Swish) activation works significantly better.However, we offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.\n",
    "\n",
    "The **SwiGLU** variant is defined as:\n",
    "\n",
    "$$\\text{SwiGLU}(x, {W_1}, {W_2}) = \\text{SiLU}({W_1}x) \\otimes ({W_2}x)$$\n",
    "\n",
    "In this version, the \"gate\" isn't just a 0-to-1 filter; itâ€™s a smooth, non-monotonic function that allows the network to learn much more complex representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d54ed",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aaf0d6",
   "metadata": {},
   "source": [
    "### Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c75ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None,\n",
    "    )-> None:\n",
    "        \n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.w1_weight = Parameter(\n",
    "            torch.empty((self.d_ff, self.d_model), **factory_kwargs)\n",
    "        )\n",
    "        self.w2_weight = Parameter(\n",
    "            torch.empty((self.d_model, self.d_ff), **factory_kwargs)\n",
    "        )\n",
    "        self.w3_weight = Parameter(\n",
    "            torch.empty((self.d_ff, self.d_model), **factory_kwargs)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.trunc_normal_(self.w1_weight)\n",
    "        nn.init.trunc_normal_(self.w2_weight)\n",
    "        nn.init.trunc_normal_(self.w3_weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = x @ self.w1_weight.T\n",
    "        x3 = x @ self.w3_weight.T\n",
    "        gated = F.silu(x3) * x1\n",
    "        result = gated @ self.w2_weight.T\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b98a1",
   "metadata": {},
   "source": [
    "### Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a50bd",
   "metadata": {},
   "source": [
    "#### Class initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352be516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None,\n",
    "    )-> None:\n",
    "        \n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        # if the glue code provides `d_ff`, you do not need calculation\n",
    "        # if it doesn't, you have to define d_ff by your self: \n",
    "        # d_ff = round(d_model * 8 / 3)\n",
    "\n",
    "        # Be careful the order of d_ff & d_model, you should leverage row-major\n",
    "        self.w1_weight = Parameter(\n",
    "            torch.empty((self.d_ff, self.d_model), **factory_kwargs)\n",
    "        )\n",
    "        self.w2_weight = Parameter(\n",
    "            torch.empty((self.d_model, self.d_ff), **factory_kwargs)\n",
    "        )\n",
    "        self.w3_weight = Parameter(\n",
    "            torch.empty((self.d_ff, self.d_model), **factory_kwargs)\n",
    "        )\n",
    "        self.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to initialzie each weight\n",
    "def reset_parameters(self) -> None:\n",
    "    nn.init.trunc_normal_(self.w1_weight)\n",
    "    nn.init.trunc_normal_(self.w2_weight)\n",
    "    nn.init.trunc_normal_(self.w3_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST be ONLY calculation \n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x @ self.w1_weight.T\n",
    "    x3 = x @ self.w3_weight.T\n",
    "    gated = F.silu(x3) * x1\n",
    "    result = gated @ self.w2_weight.T\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "This code was wrong\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "    self.w1_weight = Linear(self.d_model, self.d_ff) \n",
    "    self.w2_weight = Linear(self.d_ff, self.d_model) \n",
    "    self.w3_weight = Linear(self.d_model, self.d_ff) \n",
    "    return self.w2_weight(SiLU(self.w1_weight(x)) * self.w3_weight(x))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a902d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_swiglu(\n",
    "    d_model: int,\n",
    "    d_ff: int,\n",
    "    w1_weight: Float[Tensor, \" d_ff d_model\"],\n",
    "    w2_weight: Float[Tensor, \" d_model d_ff\"],\n",
    "    w3_weight: Float[Tensor, \" d_ff d_model\"],\n",
    "    in_features: Float[Tensor, \" ... d_model\"],\n",
    ") -> Float[Tensor, \" ... d_model\"]:\n",
    "    \"\"\"Given the weights of a SwiGLU network, return\n",
    "    the output of your implementation with these weights.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the feedforward input and output.\n",
    "        d_ff (int): Dimensionality of the up-project happening internally to your swiglu.\n",
    "\n",
    "        w1_weight (Float[Tensor, \"d_ff d_model\"]): Stored weights for W1\n",
    "        w2_weight (Float[Tensor, \"d_model d_ff\"]): Stored weights for W2\n",
    "        w3_weight (Float[Tensor, \"d_ff d_model\"]): Stored weights for W3\n",
    "        -> this indicating the shape and dtype for eacb weight, so for example\n",
    "           w1 = Float number, data = Tensor, Shape = (d_ff, d_model)\n",
    "\n",
    "        in_features (Float[Tensor, \"... d_model\"]): Input embeddings to the feed-forward layer.\n",
    "        -> input data info.  '...' means any number of leading dimension\n",
    "\n",
    "    Returns:\n",
    "        Float[Tensor, \"... d_model\"]: Output embeddings of the same shape as the input embeddings.\n",
    "        -> output of SwiGLUFFN\n",
    "\n",
    "    \"\"\"\n",
    "    # define the FFN shape:\n",
    "    swinglu = SwiGLUFFN(d_model, d_ff)\n",
    "\n",
    "    # Set your parameters in `swinglu`\n",
    "    # Example: If your state dict keys match, you can use `load_state_dict()`\n",
    "    # swiglu.load_state_dict(weights)\n",
    "    # You can also manually assign the weights\n",
    "    # since w1_weight is already a Parameter, you need to add .data after w1_weight\n",
    "    # otherwise Pytorch will not track w1_weight as parameter any more.\n",
    "    swinglu.w1_weight.data = w1_weight\n",
    "    swinglu.w2_weight.data = w2_weight\n",
    "    swinglu.w3_weight.data = w3_weight\n",
    "\n",
    "    # then input your data\n",
    "    return swinglu(in_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
