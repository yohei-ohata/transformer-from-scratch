{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71507c66",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ced232",
   "metadata": {},
   "source": [
    "* [Tiktokens](https://tiktokenizer.vercel.app/)\n",
    "* [Hugging Face Tokenizer docs](https://huggingface.co/docs/transformers/v5.1.0/en/fast_tokenizers)\n",
    "* [Stanford Assignment1](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db989d5",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30afd2",
   "metadata": {},
   "source": [
    "#### Tokenizer: Strings <-> Tokens (indices)\n",
    "\n",
    "Tokenization is a foundational NLP step that breaks text into smaller units (tokens) so machine learning models can process it. <br>\n",
    "Why it matters?: A Transformers model expects the input to be a PyTorch or NumPy tensor. strings -> tokens -> tensors -> transformers\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "input strings: Hello, This is CiNet, NICT.\n",
    "output tokens: 13225, 11, 1328, 382, 21572, 9944, 11, 478, 28594, 13\n",
    "\n",
    "```\n",
    "#### Methods\n",
    "* Word-Based Tokenization\n",
    "* Character-Based Tokenization\n",
    "* Byte-Based Tokenization\n",
    "\n",
    "##### 1. Word-Based Tokenization\n",
    "\n",
    "This method splits text using delimiters(definition) such as spaces and punctuation.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```\n",
    "\"Tokenization is great\"\n",
    "â†’ [\"Tokenization\", \"is\", \"great\"]\n",
    "```\n",
    "* âœ… Pros\n",
    "    * Intuitive and human-readable\n",
    "    * Each token usually carries semantic meaning\n",
    "\n",
    "* âŒ Cons\n",
    "    * **Enormous vocabulary**: Every unique word must be stored, often resulting in hundreds of thousands of tokens\n",
    "    * **Out-of-Vocabulary (OOV) issues**: Cannot handle unseen words (e.g., typos, slang, new words)\n",
    "    * **Data sparsity**: Related words like *run* and *running* are treated as completely different tokens\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Character-Based Tokenization\n",
    "\n",
    "This method splits text into individual characters (letters, digits, punctuation, etc.).\n",
    "\n",
    "**Example**\n",
    "\n",
    "```\n",
    "\"Token\"\n",
    "â†’ [\"T\", \"o\", \"k\", \"e\", \"n\"]\n",
    "```\n",
    "\n",
    "* âœ… Pros\n",
    "    * **Very small vocabulary**: About 256 possible characters in basic English\n",
    "    * **No OOV problem**: Any word can be constructed from characters\n",
    "\n",
    "* âŒ Cons\n",
    "    * **Very long sequences**: Leads to high computational cost (especially for Transformers)\n",
    "    * **Low semantic meaning**: Individual characters carry little meaning on their own\n",
    "\n",
    "#### 3. Byte-Based (Byte-Level) Tokenization\n",
    "\n",
    "This method treats text as raw UTF-8 bytes instead of characters. Even complex characters (e.g., emojis or foreign scripts) are decomposed into byte sequences.\n",
    "\n",
    "**Example**\n",
    "\n",
    "* A 4-byte emoji is split into 4 individual bytes.\n",
    "\n",
    "* âœ… Pros\n",
    "    * **Language-agnostic**: Works well for multilingual text\n",
    "    * **Robust to rare characters and emojis**\n",
    "    * **No OOV problem**\n",
    "\n",
    "* âŒ Cons\n",
    "    * Can produce long token sequences\n",
    "    * Often requires additional compression (e.g., BPE) for efficiency\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Comparison\n",
    "\n",
    "| Feature          | Word-Based   | Character-Based   | Byte-Based        |\n",
    "| ---------------- | ------------ | ----------------- | ----------------- |\n",
    "| Granularity      | High (Words) | Low (Characters)  | Very Low (Bytes)  |\n",
    "| Vocabulary Size  | Very Large   | Very Small (~256) | Very Small (~256) |\n",
    "| OOV Handling     | Poor         | Excellent         | Excellent         |\n",
    "| Sequence Length  | Short        | Very Long         | Very Long         |\n",
    "| Semantic Meaning | High         | Low               | Low               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd4ade",
   "metadata": {},
   "source": [
    "## Bite-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3091e0f",
   "metadata": {},
   "source": [
    "### BPE Encoding (Training & Encoding)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "**Input string**\n",
    "\n",
    "`\"the cat ate the rat\"`\n",
    "\n",
    "**Initial Vocabulary**\n",
    "\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r'}`\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 â€” Pre-tokenization\n",
    "\n",
    "The input is split into space-aware chunks:\n",
    "\n",
    "`\"the cat ate the rat\"  â†’  ['the', ' cat', ' ate', ' the', ' rat']`\n",
    "\n",
    "Notice:\n",
    "\n",
    "* Spaces stay attached to the following word.\n",
    "* This is typical for GPT-style tokenizers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2 â€” Apply BPE Training (Learning Merges)\n",
    "\n",
    "We now simulate **training**, where we repeatedly:\n",
    "\n",
    "1. Count all adjacent token pairs\n",
    "2. Merge the most frequent pair\n",
    "3. Update the corpus\n",
    "4. Repeat\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”¹ Iteration 1 â€” Count Adjacent Pairs\n",
    "\n",
    "From the full corpus, we count all adjacent pairs:\n",
    "\n",
    "| Pair   | Frequency |\n",
    "| ------ | --------- |\n",
    "| (a, t) | 3         |\n",
    "| (t, h) | 2         |\n",
    "| (h, e) | 2         |\n",
    "| (c, a) | 1         |\n",
    "| (r, a) | 1         |\n",
    "| (t, e) | 1         |\n",
    "| ( , c) | 1         |\n",
    "| ( , a) | 1         |\n",
    "| ( , t) | 1         |\n",
    "| ( , r) | 1         |\n",
    "\n",
    "* ðŸ† Most Frequent Pair: `(a, t) â†’ frequency 3`\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”¹ Merge Rule #1**\n",
    "\n",
    "`a + t â†’ at`\n",
    "\n",
    "**Updated Corpus**\n",
    "\n",
    "`[\"t\",\"h\",\"e\"], [\" \",\"c\",\"at\"], [\" \",\"at\",\"e\"], [\"t\",\"h\",\"e\"], [\" \",\"r\",\"at\"]`\n",
    "\n",
    "**Updated Vocabulary**\n",
    "\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r', 7: b'at'}`\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”¹ Iteration 2 â€” Recalculate Frequencies\n",
    "\n",
    "Now count again:\n",
    "\n",
    "| Pair    | Frequency |\n",
    "| ------- | --------- |\n",
    "| (t, h)  | 2         |\n",
    "| (h, e)  | 2         |\n",
    "| ( , at) | 1         |\n",
    "| (c, at) | 1         |\n",
    "| (r, at) | 1         |\n",
    "| (at, e) | 1         |\n",
    "| ( , c)  | 1         |\n",
    "| ( , r)  | 1         |\n",
    "\n",
    "* ðŸ† Most Frequent Pair: `(t, h) â†’ frequency 2`\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”¹ Merge Rule #2**\n",
    "\n",
    "`t + h â†’ th`\n",
    "\n",
    "**Updated Corpus**\n",
    "\n",
    "`[\"th\",\"e\"], [\" \",\"c\",\"at\"], [\" \",\"at\",\"e\"], [\"th\",\"e\"], [\" \",\"r\",\"at\"]`\n",
    "\n",
    "**Updated Vocabulary**\n",
    "\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r', 7: b'at', 8: b'th'}`\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”¹ Iteration 3 â€” Recalculate Frequencies\n",
    "\n",
    "Now count again:\n",
    "\n",
    "| Pair    | Frequency |\n",
    "| ------- | --------- |\n",
    "| (th, e) | 2         |\n",
    "| ( , at) | 1         |\n",
    "| (c, at) | 1         |\n",
    "| (r, at) | 1         |\n",
    "| (at, e) | 1         |\n",
    "| ( , c)  | 1         |\n",
    "| ( , r)  | 1         |\n",
    "\n",
    "* ðŸ† Most Frequent Pair: `(th, e) â†’ frequency 2`\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”¹ Merge Rule #3**\n",
    "\n",
    "`th + e â†’ the`\n",
    "\n",
    "**Updated Corpus**\n",
    "\n",
    "`[\"the\"], [\" \",\"c\",\"at\"], [\" \",\"at\",\"e\"], [\"the\"], [\" \",\"r\",\"at\"]`\n",
    "\n",
    "**Updated Vocabulary**\n",
    "\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r', 7: b'at', 8: b'th', 9: b'the'}`\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”¹ Stop Condition\n",
    "\n",
    "Now the highest frequency is 1 for all pairs.\n",
    "Training usually continues until:\n",
    "\n",
    "* A target vocabulary size is reached\n",
    "* Or no pair appears more than once\n",
    "\n",
    "For this example, we stop here.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Learned Merges**\n",
    "\n",
    "1. (a, t) â†’ at\n",
    "2. (t, h) â†’ th\n",
    "3. (th, e) â†’ the\n",
    "\n",
    "---\n",
    "\n",
    "**Final Vocabulary**\n",
    "\n",
    "`{0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r', 7: b'at', 8: b'th', 9: b'the'}`\n",
    "\n",
    "---\n",
    "\n",
    "**Now Encoding the Sentence**\n",
    "\n",
    "Using the learned merges:\n",
    "\n",
    "`['the', ' cat', ' ate', ' the', ' rat']`\n",
    "\n",
    "Becomes:\n",
    "\n",
    "`[b'the'], [b' ', b'c', b'at'], [b' ', b'at', b'e'], [b'the'], [b' ', b'r', b'at']`\n",
    "\n",
    "Mapped to IDs:\n",
    "\n",
    "`[9, 0, 2, 7, 0, 7, 3, 9, 0, 6, 7]`\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”Ž What This Shows\n",
    "\n",
    "* BPE builds frequent subwords automatically.\n",
    "* \"the\" becomes a single token.\n",
    "* \"at\" becomes reusable for cat / ate / rat.\n",
    "* Rare patterns stay broken into smaller units.\n",
    "\n",
    "This is why BPE balances:\n",
    "\n",
    "* Efficiency\n",
    "* Small vocabulary\n",
    "* No OOV problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580431e",
   "metadata": {},
   "source": [
    "#### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "\n",
    "    # 1. Initialize Vocab\n",
    "    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "    next_id = 256\n",
    "\n",
    "    special_token_bytes = [s.encode(\"utf-8\") for s in special_tokens]\n",
    "    for st in special_token_bytes:\n",
    "        if st not in vocab.values():\n",
    "            vocab[next_id] = st\n",
    "            next_id += 1\n",
    "\n",
    "    # 2. Build Word Frequencies\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "\n",
    "    # Split around special tokens\n",
    "    segments = [raw_bytes]\n",
    "    for st in special_token_bytes:\n",
    "        new_segments = []\n",
    "        for seg in segments:\n",
    "            parts = seg.split(st)\n",
    "            for i, part in enumerate(parts):\n",
    "                if part: new_segments.append(part)\n",
    "                if i < len(parts) - 1: new_segments.append(st)\n",
    "        segments = new_segments\n",
    "\n",
    "    word_freqs = Counter()\n",
    "    for seg in segments:\n",
    "        if seg in special_token_bytes:\n",
    "            word_freqs[(seg,)] += 1\n",
    "        else:\n",
    "            # Decode to run regex, but keep it as a sequence of bytes\n",
    "            decoded = seg.decode(\"utf-8\", errors=\"ignore\")\n",
    "            for chunk in re.findall(PAT, decoded):\n",
    "                # Represent word as a list of individual byte-tokens\n",
    "                word = tuple(bytes([b]) for b in chunk.encode(\"utf-8\"))\n",
    "                word_freqs[word] += 1\n",
    "\n",
    "    # 3. Efficient State Tracking\n",
    "    unique_words = [list(w) for w in word_freqs.keys()]\n",
    "    counts = list(word_freqs.values())\n",
    "    \n",
    "    pair_counts = defaultdict(int)\n",
    "    pair_to_word_indices = defaultdict(set)\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        for pair in zip(word, word[1:]):\n",
    "            pair_counts[pair] += counts[i]\n",
    "            pair_to_word_indices[pair].add(i)\n",
    "\n",
    "    merges: List[Tuple[bytes, bytes]] = []\n",
    "\n",
    "    # 4. Main Merge Loop\n",
    "    while len(vocab) < vocab_size:\n",
    "        if not pair_counts:\n",
    "            break\n",
    "        \n",
    "        best_pair = max(pair_counts.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "        if pair_counts[best_pair] <= 0:\n",
    "            break\n",
    "\n",
    "        merges.append(best_pair)\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "\n",
    "        affected_word_indices = pair_to_word_indices.pop(best_pair)\n",
    "        pair_counts.pop(best_pair)\n",
    "\n",
    "        for i in affected_word_indices:\n",
    "            word = unique_words[i]\n",
    "            freq = counts[i]\n",
    "            \n",
    "            j = 0\n",
    "            new_word = []\n",
    "            while j < len(word):\n",
    "                if j < len(word) - 1 and (word[j], word[j+1]) == best_pair:\n",
    "                    \n",
    "                    if j > 0:\n",
    "                        prev_pair = (word[j-1], word[j])\n",
    "                        pair_counts[prev_pair] -= freq\n",
    "                    if j < len(word) - 2:\n",
    "                        next_pair = (word[j+1], word[j+2])\n",
    "                        pair_counts[next_pair] -= freq\n",
    "                    \n",
    "                    new_word.append(new_token)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    new_word.append(word[j])\n",
    "                    j += 1\n",
    "            \n",
    "            unique_words[i] = new_word\n",
    "            for j in range(len(new_word) - 1):\n",
    "                pair = (new_word[j], new_word[j+1])\n",
    "                if new_word[j] == new_token or new_word[j+1] == new_token:\n",
    "                    pair_counts[pair] += freq\n",
    "                    pair_to_word_indices[pair].add(i)\n",
    "\n",
    "        pair_counts = defaultdict(int, {k: v for k, v in pair_counts.items() if v > 0})\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf52e30",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import regex as re\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator, Any, ClassVar\n",
    "\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def bytes_to_unicode() -> Dict[int, str]:\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + \\\n",
    "         list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1)) + \\\n",
    "         list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(256):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(256 + n)\n",
    "            n += 1\n",
    "    return {b: chr(c) for b, c in zip(bs, cs)}\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    pat_str: ClassVar[str] = PAT\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Dict[int, bytes],\n",
    "        merges: List[Tuple[bytes, bytes]],\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ):\n",
    "        self.id_to_token = dict(vocab)\n",
    "        self.token_to_id = {v: k for k, v in self.id_to_token.items()}\n",
    "        self.ranks = {pair: i for i, pair in enumerate(merges)}\n",
    "        \n",
    "        # Pre-compile the PAT regex once\n",
    "        self.compiled_pat = re.compile(self.pat_str)\n",
    "        \n",
    "        self.special_tokens = special_tokens or []\n",
    "        if self.special_tokens:\n",
    "            special_pat = \"|\".join(re.escape(st) for st in sorted(self.special_tokens, key=len, reverse=True))\n",
    "            self.special_regex = re.compile(f\"({special_pat})\")\n",
    "        else:\n",
    "            self.special_regex = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: List[str] | None = None) -> \"Tokenizer\":\n",
    "        with open(vocab_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_json = json.load(f)\n",
    "            vocab = {int(k): v.encode(\"latin1\") for k, v in vocab_json.items()}\n",
    "\n",
    "        # Build the decoder dictionary ONCE here\n",
    "        byte_encoder = bytes_to_unicode()\n",
    "        symbol_to_byte = {v: k for k, v in byte_encoder.items()}\n",
    "\n",
    "        merges = []\n",
    "        with open(merges_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith(\"#version\"):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                # Use the local decoder dictionary\n",
    "                t1 = bytes([symbol_to_byte[char] for char in parts[0]])\n",
    "                t2 = bytes([symbol_to_byte[char] for char in parts[1]])\n",
    "                merges.append((t1, t2))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_symbol(symbol: str) -> int:\n",
    "        decoder = {v: k for k, v in bytes_to_unicode().items()}\n",
    "        return decoder[symbol]\n",
    "\n",
    "    def _apply_bpe(self, token_bytes: bytes) -> List[bytes]:\n",
    "        if len(token_bytes) <= 1:\n",
    "            return [token_bytes]\n",
    "            \n",
    "        parts = [bytes([b]) for b in token_bytes]\n",
    "        \n",
    "        while len(parts) > 1:\n",
    "            # Find all possible adjacent pairs in the current sequence\n",
    "            pairs = [(parts[i], parts[i+1]) for i in range(len(parts)-1)]\n",
    "            \n",
    "            # Find the pair with the best (lowest) rank\n",
    "            # Pairs not in ranks are ignored (float('inf'))\n",
    "            best_pair = min(pairs, key=lambda p: self.ranks.get(p, float('inf')))\n",
    "            \n",
    "            if best_pair not in self.ranks:\n",
    "                break\n",
    "                \n",
    "            # Perform the merge: replace all occurrences of best_pair in this sequence\n",
    "            new_parts = []\n",
    "            i = 0\n",
    "            while i < len(parts):\n",
    "                if i < len(parts) - 1 and (parts[i], parts[i+1]) == best_pair:\n",
    "                    new_parts.append(parts[i] + parts[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_parts.append(parts[i])\n",
    "                    i += 1\n",
    "            parts = new_parts\n",
    "        return parts\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        if self.special_regex:\n",
    "            segments = [p for p in self.special_regex.split(text) if p]\n",
    "        else:\n",
    "            segments = [text]\n",
    "\n",
    "        ids = []\n",
    "        for seg in segments:\n",
    "            if seg in self.special_tokens:\n",
    "                ids.append(self.token_to_id[seg.encode(\"utf-8\")])\n",
    "                continue\n",
    "            \n",
    "            # Use the pre-compiled regex\n",
    "            pieces = self.compiled_pat.findall(seg)\n",
    "            \n",
    "            for piece in pieces:\n",
    "                piece_bytes = piece.encode(\"utf-8\")\n",
    "                for tok in self._apply_bpe(piece_bytes):\n",
    "                    ids.append(self.token_to_id[tok])\n",
    "        return ids\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for text in iterable:\n",
    "            for token_id in self.encode(text):\n",
    "                yield token_id\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        byte_sequence = b\"\".join(self.id_to_token[i] for i in ids)\n",
    "        return byte_sequence.decode(\"utf-8\", errors=\"replace\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
