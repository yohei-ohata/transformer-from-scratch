{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71507c66",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf3f8e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fbee0",
   "metadata": {},
   "source": [
    "The **Embedding Layer** serves as the entry point of the Transformer, <br>\n",
    "acting as a lookup table that translates discrete, categorical data (integers) into a continuous, <br>\n",
    "high-dimensional space where the model can perform \"meaning-based\" mathematics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db989d5",
   "metadata": {},
   "source": [
    "### Implimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5dbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        device: torch.device | None=None,\n",
    "        dtype: torch.dtype | None=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.empty((num_embeddings, embedding_dim), **factory_kwargs)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.trunc_normal_(self.weight)\n",
    "    \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weight[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6426eb",
   "metadata": {},
   "source": [
    "## Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d7b59",
   "metadata": {},
   "source": [
    "#### Definition `Linear` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        device: torch.device | None=None,\n",
    "        dtype: torch.dtype | None=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.empty((num_embeddings, embedding_dim), **factory_kwargs)\n",
    "        )\n",
    "        self.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0b56e",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6adfcf",
   "metadata": {},
   "source": [
    "**`trunc_normal_`** initialization\n",
    "\n",
    "Same as `Linear` See details in `linear_layer.ipynb` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca17b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self) -> None:\n",
    "    nn.init.trunc_normal_(self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ff44e",
   "metadata": {},
   "source": [
    "### `Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "    return self.weight[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dba790",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0395cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([11])\n",
      "Input Tensor (Token IDs):\n",
      "tensor([9, 0, 2, 7, 0, 7, 3, 9, 0, 6, 7])\n",
      "\n",
      "Weight Shape (Vocab Size x Embedding Dim): torch.Size([10, 4])\n",
      "Weight Tensor (The Lookup Table):\n",
      "Parameter containing:\n",
      "tensor([[-0.4136,  0.1851, -0.7171, -0.7127],\n",
      "        [-1.0888,  0.2085, -1.3409, -0.0901],\n",
      "        [ 1.5916,  0.6708, -0.2307, -0.5711],\n",
      "        [-0.2284,  0.2545, -0.6289,  0.9571],\n",
      "        [ 0.2058,  0.0875, -0.0067,  0.4083],\n",
      "        [-1.0441, -1.1364, -1.2963,  0.3038],\n",
      "        [ 0.9817, -0.9500, -0.8207,  1.0706],\n",
      "        [ 0.3841, -0.4254, -1.2210,  1.8807],\n",
      "        [ 0.4021, -1.1038, -1.1837,  0.3225],\n",
      "        [ 0.9744, -0.6042, -0.2462, -0.3535]], requires_grad=True)\n",
      "\n",
      "Output Shape (Sequence Length x Embedding Dim): torch.Size([11, 4])\n",
      "Output Tensor (The Looked-up Vectors):\n",
      "tensor([[ 0.9744, -0.6042, -0.2462, -0.3535],\n",
      "        [-0.4136,  0.1851, -0.7171, -0.7127],\n",
      "        [ 1.5916,  0.6708, -0.2307, -0.5711],\n",
      "        [ 0.3841, -0.4254, -1.2210,  1.8807],\n",
      "        [-0.4136,  0.1851, -0.7171, -0.7127],\n",
      "        [ 0.3841, -0.4254, -1.2210,  1.8807],\n",
      "        [-0.2284,  0.2545, -0.6289,  0.9571],\n",
      "        [ 0.9744, -0.6042, -0.2462, -0.3535],\n",
      "        [-0.4136,  0.1851, -0.7171, -0.7127],\n",
      "        [ 0.9817, -0.9500, -0.8207,  1.0706],\n",
      "        [ 0.3841, -0.4254, -1.2210,  1.8807]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Setup dimensions\n",
    "vocab_size = 10  \n",
    "# {0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't', 6: b'r', 7: b'at', 8: b'th', 9: b'the'}\n",
    "d_model = 4      \n",
    "\n",
    "# 2. Initialize the module\n",
    "# This creates a weight matrix of shape (10, 4)\n",
    "embedding_layer = Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "# 3. Define your encoded input (the token IDs)\n",
    "token_ids = torch.tensor([9, 0, 2, 7, 0, 7, 3, 9, 0, 6, 7])\n",
    "\n",
    "# 4. Run the forward pass\n",
    "output = embedding_layer(token_ids)\n",
    "\n",
    "# 5. Check the results\n",
    "print(f\"Input Shape:  {token_ids.shape}\")\n",
    "print(\"Input Tensor (Token IDs):\")\n",
    "print(token_ids)\n",
    "\n",
    "print(f\"\\nWeight Shape (Vocab Size x Embedding Dim): {embedding_layer.weight.shape}\")\n",
    "print(\"Weight Tensor (The Lookup Table):\")\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "print(f\"\\nOutput Shape (Sequence Length x Embedding Dim): {output.shape}\")\n",
    "print(\"Output Tensor (The Looked-up Vectors):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa6e86",
   "metadata": {},
   "source": [
    "## Optimize Embedding Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ce523",
   "metadata": {},
   "source": [
    "The embedding weights are optimized through **Backpropagation**. <br>\n",
    "Initially, the vectors in your  matrix are random noise. <br>\n",
    "Over time, the model \"nudges\" these numbers so that words used in similar contexts end up with similar vectors. <br>\n",
    "\n",
    "Here is the overview:\n",
    "\n",
    "---\n",
    "\n",
    "**1. The Forward Pass (The Guess)**\n",
    "\n",
    "The model takes your input `the cat ate the...`, converts them to vectors using the current weights, and predicts the next token ID.\n",
    "\n",
    "* **Input:** `[9, 0, 2, 7, 0, 7, 3, 9, 0]` (\"the cat ate the \")\n",
    "* **Target:** `6` (ID for \"r\" in \"rat\")\n",
    "* **Model Prediction:** It might guess ID `1` (\"a\") with high confidence.\n",
    "\n",
    "**2. The Loss Calculation & Backpropagation**\n",
    "\n",
    "We compare the model's guess to the actual target using a **Loss Function** (usually Cross-Entropy).\n",
    "\n",
    "* If the model guessed \"a\" but the answer was \"r\", the \"Loss\" is high.\n",
    "* The gradient travels backward through the network until it hits the **Embedding Layer**.\n",
    "* Since the embedding layer is just a lookup table, the optimization is very specific: **only the rows (vectors) used in that sentence get updated.**\n",
    "* e.g.: If token ID `1` was used, its corresponding row in the weight matrix receives a gradient.\n",
    "\n",
    "The update for a specific weight follows this logic:\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\nabla L\n",
    "$$\n",
    "\n",
    "*  $\\eta$: The **Learning Rate** (how big of a step we take).\n",
    "*  $\\nabla L$: The direction of the error.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
